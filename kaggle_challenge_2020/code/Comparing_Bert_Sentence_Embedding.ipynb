{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New_Submission_Taha_Method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_p0wG784I1V",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYuIznd6Nce0",
        "colab_type": "code",
        "outputId": "93746658-5734-4ce1-bd27-9f244ac5545e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/', force_remount=True) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLiCVyz0OhlE",
        "colab_type": "code",
        "outputId": "e7362d29-9ff5-4d46-b076-55a90a1daa1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/gdrive/My Drive/Altegrad' "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Altegrad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Dvh-AHukLe",
        "colab_type": "code",
        "outputId": "ecf4e392-658a-4404-95c8-56837e9d9f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!pip install unidecode "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 3.3MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlIvvp_CQjOI",
        "colab_type": "code",
        "outputId": "6bed759b-a795-4d32-b32c-50494e9cf505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import codecs\n",
        "import sys\n",
        "import csv\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "data_path = \"./text/\"\n",
        "edgelist_path = \"./data/edgelist.txt\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-uDee7I6wf8",
        "colab_type": "text"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkuX61jVQ0Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph():\n",
        "    '''Function that build a directed weighted graph from the edgelist.txt'''\n",
        "    G = nx.read_weighted_edgelist(edgelist_path, create_using=nx.DiGraph())\n",
        "    print(\"Number of nodes : \", G.number_of_nodes())\n",
        "    print(\"Number of edges : \", G.number_of_edges())\n",
        "    return G\n",
        "\n",
        "def build_train_test(train_path, test_path):\n",
        "    \"\"\"Function that reads the train.csv and returns the train Ids and train labels\n",
        "        and reads the test.csv and returns the test Ids\n",
        "    \"\"\"\n",
        "    with open(train_path, 'r') as f:\n",
        "        train_data = f.read().splitlines()\n",
        "        \n",
        "    train_hosts = list()\n",
        "    y_train = list()\n",
        "    for row in train_data:\n",
        "        host, label = row.split(\",\")\n",
        "        train_hosts.append(host)\n",
        "        y_train.append(label.lower())\n",
        "        \n",
        "    df_train = pd.DataFrame(data= y_train, index = train_hosts, columns= [\"class\"]).reset_index()\n",
        "    \n",
        "    with open(test_path, 'r') as f:\n",
        "        test_hosts = f.read().splitlines()\n",
        "    df_test =  pd.DataFrame(data=[] , index = test_hosts, columns= [\"class\"]).reset_index()\n",
        "    return df_train, df_test\n",
        "\n",
        "def write_submission(write_path, test_hosts, model_classes_list, predicted_probas):\n",
        "    \"\"\"Function that writes the submission file\n",
        "  there is a need to be pass it  : \n",
        "    - The path of the file to create\n",
        "    - The test Ids (returned by build_train_test)\n",
        "    - The classes labels as a list\n",
        "    - The predicted probas for those class labels (same order)\n",
        "    \"\"\"\n",
        "    with open(write_path, 'w') as csvfile:\n",
        "        writer = csv.writer(csvfile, delimiter=',')\n",
        "        model_classes_list.insert(0, \"Host\")\n",
        "        writer.writerow(model_classes_list)\n",
        "        for i,test_host in enumerate(test_hosts):\n",
        "            lst = predicted_probas[i,:].tolist()\n",
        "            lst.insert(0, test_host)\n",
        "            writer.writerow(lst)\n",
        "\n",
        "def text_from_id(id):\n",
        "    id = str(id)\n",
        "    try :\n",
        "        with codecs.open(data_path+id, 'r', encoding=\"utf-8\") as f:\n",
        "            text = f.readlines()\n",
        "    except:\n",
        "        with codecs.open(data_path+id, 'r', encoding=\"latin-1\") as f:\n",
        "            text = f.readlines()\n",
        "    return text\n",
        "\n",
        "def build_local_test(train_hosts, y_train, size_local_test=.25):\n",
        "    \n",
        "    local_train, local_test, local_y_train, local_y_test = train_test_split(train_hosts, y_train,\n",
        "                                                                            stratify=y_train, \n",
        "                                                                            test_size=size_local_test)\n",
        "    \n",
        "    return local_train, local_y_train, local_test, local_y_test\n",
        "\n",
        "def compute_score(predictions, y_true, classes_order):\n",
        "    dico = {v:k for k, v in enumerate(classes_order)}\n",
        "    print(dico)\n",
        "    loss = 0\n",
        "    for i, cla in enumerate(y_true) :\n",
        "        loss -= np.log(predictions[i, dico[cla]])\n",
        "    loss = loss/len(y_true)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZDtIjrA5ULm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv('./embeds/' + 'test.csv', header = None)\n",
        "test_data.columns = ['File']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KscM_iJl9WsX",
        "colab_type": "text"
      },
      "source": [
        "### Use Bert Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiQlHfXFJ5hn",
        "colab_type": "code",
        "outputId": "f83b4e36-fee2-468e-e330-584d97557d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "train_camembert = pd.read_csv('./embeds/embeds_taha.csv') \n",
        "test_camembert = pd.read_csv('./embeds/test_camembert.csv') \n",
        "\n",
        "train_camembert.head() "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004979</td>\n",
              "      <td>0.051084</td>\n",
              "      <td>0.011548</td>\n",
              "      <td>0.144239</td>\n",
              "      <td>-0.054896</td>\n",
              "      <td>-0.091976</td>\n",
              "      <td>-0.079737</td>\n",
              "      <td>0.158545</td>\n",
              "      <td>-0.063696</td>\n",
              "      <td>0.029545</td>\n",
              "      <td>-0.028441</td>\n",
              "      <td>0.035867</td>\n",
              "      <td>-0.028134</td>\n",
              "      <td>0.150745</td>\n",
              "      <td>0.162346</td>\n",
              "      <td>-0.035347</td>\n",
              "      <td>-0.001045</td>\n",
              "      <td>-0.116933</td>\n",
              "      <td>0.137100</td>\n",
              "      <td>-0.052911</td>\n",
              "      <td>0.046483</td>\n",
              "      <td>-0.012581</td>\n",
              "      <td>0.034688</td>\n",
              "      <td>-0.209713</td>\n",
              "      <td>0.126438</td>\n",
              "      <td>-0.194954</td>\n",
              "      <td>-0.005686</td>\n",
              "      <td>0.055490</td>\n",
              "      <td>-0.018563</td>\n",
              "      <td>0.043815</td>\n",
              "      <td>0.024498</td>\n",
              "      <td>-0.170535</td>\n",
              "      <td>0.127708</td>\n",
              "      <td>0.183329</td>\n",
              "      <td>-0.058293</td>\n",
              "      <td>0.028888</td>\n",
              "      <td>0.003619</td>\n",
              "      <td>0.127561</td>\n",
              "      <td>0.032685</td>\n",
              "      <td>-0.005017</td>\n",
              "      <td>...</td>\n",
              "      <td>0.112802</td>\n",
              "      <td>0.088039</td>\n",
              "      <td>0.314132</td>\n",
              "      <td>0.163118</td>\n",
              "      <td>-0.062759</td>\n",
              "      <td>-0.075335</td>\n",
              "      <td>0.011383</td>\n",
              "      <td>0.013771</td>\n",
              "      <td>-0.061509</td>\n",
              "      <td>0.046618</td>\n",
              "      <td>-0.003913</td>\n",
              "      <td>-0.064163</td>\n",
              "      <td>0.053930</td>\n",
              "      <td>0.042424</td>\n",
              "      <td>0.023481</td>\n",
              "      <td>-0.163376</td>\n",
              "      <td>-0.105974</td>\n",
              "      <td>0.039036</td>\n",
              "      <td>-0.039118</td>\n",
              "      <td>-0.163902</td>\n",
              "      <td>0.032288</td>\n",
              "      <td>0.022957</td>\n",
              "      <td>0.030852</td>\n",
              "      <td>-0.239209</td>\n",
              "      <td>0.091170</td>\n",
              "      <td>-0.015870</td>\n",
              "      <td>-0.105228</td>\n",
              "      <td>-0.125584</td>\n",
              "      <td>-0.003099</td>\n",
              "      <td>-0.039667</td>\n",
              "      <td>-0.042257</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.040467</td>\n",
              "      <td>0.062530</td>\n",
              "      <td>0.063022</td>\n",
              "      <td>-0.122656</td>\n",
              "      <td>-0.116885</td>\n",
              "      <td>0.005459</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.008539</td>\n",
              "      <td>0.196301</td>\n",
              "      <td>-0.017056</td>\n",
              "      <td>-0.081347</td>\n",
              "      <td>0.046665</td>\n",
              "      <td>-0.032499</td>\n",
              "      <td>0.080598</td>\n",
              "      <td>0.025480</td>\n",
              "      <td>-0.037952</td>\n",
              "      <td>0.034152</td>\n",
              "      <td>0.106114</td>\n",
              "      <td>0.201544</td>\n",
              "      <td>-0.049484</td>\n",
              "      <td>0.059490</td>\n",
              "      <td>0.099460</td>\n",
              "      <td>-0.075301</td>\n",
              "      <td>0.074141</td>\n",
              "      <td>-0.065830</td>\n",
              "      <td>-0.072313</td>\n",
              "      <td>0.040387</td>\n",
              "      <td>0.025943</td>\n",
              "      <td>-0.075173</td>\n",
              "      <td>-0.108210</td>\n",
              "      <td>-0.191474</td>\n",
              "      <td>0.446738</td>\n",
              "      <td>-0.188546</td>\n",
              "      <td>0.064377</td>\n",
              "      <td>-0.072099</td>\n",
              "      <td>0.023127</td>\n",
              "      <td>-0.110718</td>\n",
              "      <td>-0.130323</td>\n",
              "      <td>-0.093393</td>\n",
              "      <td>-0.063916</td>\n",
              "      <td>0.146621</td>\n",
              "      <td>0.045616</td>\n",
              "      <td>-0.069296</td>\n",
              "      <td>-0.204344</td>\n",
              "      <td>0.036896</td>\n",
              "      <td>0.108239</td>\n",
              "      <td>-0.230537</td>\n",
              "      <td>...</td>\n",
              "      <td>0.113540</td>\n",
              "      <td>-0.105509</td>\n",
              "      <td>0.108981</td>\n",
              "      <td>0.152414</td>\n",
              "      <td>-0.000489</td>\n",
              "      <td>-0.108477</td>\n",
              "      <td>0.171643</td>\n",
              "      <td>-0.018050</td>\n",
              "      <td>-0.030431</td>\n",
              "      <td>-0.080140</td>\n",
              "      <td>-0.007850</td>\n",
              "      <td>0.093457</td>\n",
              "      <td>-0.027985</td>\n",
              "      <td>0.041688</td>\n",
              "      <td>-0.066457</td>\n",
              "      <td>-0.049049</td>\n",
              "      <td>0.087364</td>\n",
              "      <td>0.104405</td>\n",
              "      <td>-0.057261</td>\n",
              "      <td>0.004004</td>\n",
              "      <td>-0.134647</td>\n",
              "      <td>-0.038340</td>\n",
              "      <td>0.125589</td>\n",
              "      <td>-0.069518</td>\n",
              "      <td>0.032810</td>\n",
              "      <td>-0.129408</td>\n",
              "      <td>0.002430</td>\n",
              "      <td>-0.167985</td>\n",
              "      <td>-0.183271</td>\n",
              "      <td>0.027825</td>\n",
              "      <td>-0.048238</td>\n",
              "      <td>-0.067930</td>\n",
              "      <td>0.172509</td>\n",
              "      <td>-0.102032</td>\n",
              "      <td>0.005831</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>-0.095894</td>\n",
              "      <td>-0.107140</td>\n",
              "      <td>-0.166406</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.014586</td>\n",
              "      <td>0.010158</td>\n",
              "      <td>0.019481</td>\n",
              "      <td>0.139028</td>\n",
              "      <td>-0.069928</td>\n",
              "      <td>-0.046199</td>\n",
              "      <td>-0.043685</td>\n",
              "      <td>0.151316</td>\n",
              "      <td>-0.040847</td>\n",
              "      <td>0.036641</td>\n",
              "      <td>0.029062</td>\n",
              "      <td>0.051212</td>\n",
              "      <td>-0.044985</td>\n",
              "      <td>0.119287</td>\n",
              "      <td>0.193056</td>\n",
              "      <td>-0.060690</td>\n",
              "      <td>-0.070176</td>\n",
              "      <td>-0.121432</td>\n",
              "      <td>0.084575</td>\n",
              "      <td>-0.064783</td>\n",
              "      <td>0.040374</td>\n",
              "      <td>-0.044050</td>\n",
              "      <td>0.091162</td>\n",
              "      <td>-0.182240</td>\n",
              "      <td>0.132107</td>\n",
              "      <td>-0.152883</td>\n",
              "      <td>-0.091903</td>\n",
              "      <td>0.034840</td>\n",
              "      <td>0.024752</td>\n",
              "      <td>-0.013717</td>\n",
              "      <td>-0.032962</td>\n",
              "      <td>-0.166242</td>\n",
              "      <td>0.136357</td>\n",
              "      <td>0.155130</td>\n",
              "      <td>-0.029583</td>\n",
              "      <td>0.049545</td>\n",
              "      <td>-0.060612</td>\n",
              "      <td>0.123955</td>\n",
              "      <td>0.027059</td>\n",
              "      <td>-0.036924</td>\n",
              "      <td>...</td>\n",
              "      <td>0.097372</td>\n",
              "      <td>0.096647</td>\n",
              "      <td>0.300638</td>\n",
              "      <td>0.132141</td>\n",
              "      <td>-0.123732</td>\n",
              "      <td>-0.007762</td>\n",
              "      <td>0.035959</td>\n",
              "      <td>0.007439</td>\n",
              "      <td>-0.066337</td>\n",
              "      <td>-0.012342</td>\n",
              "      <td>-0.004053</td>\n",
              "      <td>-0.046152</td>\n",
              "      <td>0.135284</td>\n",
              "      <td>0.022371</td>\n",
              "      <td>0.024677</td>\n",
              "      <td>-0.114882</td>\n",
              "      <td>-0.071554</td>\n",
              "      <td>0.016102</td>\n",
              "      <td>-0.011974</td>\n",
              "      <td>-0.173243</td>\n",
              "      <td>0.043724</td>\n",
              "      <td>0.024285</td>\n",
              "      <td>0.064807</td>\n",
              "      <td>-0.211291</td>\n",
              "      <td>0.103848</td>\n",
              "      <td>-0.010146</td>\n",
              "      <td>-0.137891</td>\n",
              "      <td>-0.148035</td>\n",
              "      <td>-0.007298</td>\n",
              "      <td>-0.061166</td>\n",
              "      <td>0.022263</td>\n",
              "      <td>-0.050842</td>\n",
              "      <td>0.149352</td>\n",
              "      <td>0.073157</td>\n",
              "      <td>0.062471</td>\n",
              "      <td>-0.108098</td>\n",
              "      <td>-0.133127</td>\n",
              "      <td>0.026859</td>\n",
              "      <td>-0.020756</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.009449</td>\n",
              "      <td>0.057803</td>\n",
              "      <td>-0.032411</td>\n",
              "      <td>0.181052</td>\n",
              "      <td>-0.023205</td>\n",
              "      <td>-0.077158</td>\n",
              "      <td>-0.088522</td>\n",
              "      <td>0.112174</td>\n",
              "      <td>-0.064386</td>\n",
              "      <td>0.011736</td>\n",
              "      <td>-0.007792</td>\n",
              "      <td>0.100436</td>\n",
              "      <td>0.003390</td>\n",
              "      <td>0.168092</td>\n",
              "      <td>0.165106</td>\n",
              "      <td>-0.055701</td>\n",
              "      <td>-0.022107</td>\n",
              "      <td>-0.087624</td>\n",
              "      <td>0.058423</td>\n",
              "      <td>-0.026992</td>\n",
              "      <td>0.035437</td>\n",
              "      <td>-0.050085</td>\n",
              "      <td>0.067647</td>\n",
              "      <td>-0.218709</td>\n",
              "      <td>0.148364</td>\n",
              "      <td>-0.155343</td>\n",
              "      <td>-0.001270</td>\n",
              "      <td>0.080315</td>\n",
              "      <td>-0.040227</td>\n",
              "      <td>-0.052534</td>\n",
              "      <td>0.019631</td>\n",
              "      <td>-0.110544</td>\n",
              "      <td>0.101820</td>\n",
              "      <td>0.177629</td>\n",
              "      <td>-0.008147</td>\n",
              "      <td>0.044479</td>\n",
              "      <td>0.016035</td>\n",
              "      <td>0.097301</td>\n",
              "      <td>0.023910</td>\n",
              "      <td>-0.030328</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066725</td>\n",
              "      <td>0.087909</td>\n",
              "      <td>0.215426</td>\n",
              "      <td>0.135860</td>\n",
              "      <td>-0.078239</td>\n",
              "      <td>-0.063461</td>\n",
              "      <td>0.016133</td>\n",
              "      <td>0.024904</td>\n",
              "      <td>-0.032214</td>\n",
              "      <td>-0.042499</td>\n",
              "      <td>-0.012666</td>\n",
              "      <td>0.030743</td>\n",
              "      <td>0.090259</td>\n",
              "      <td>0.012712</td>\n",
              "      <td>0.034152</td>\n",
              "      <td>-0.061917</td>\n",
              "      <td>-0.043080</td>\n",
              "      <td>0.021741</td>\n",
              "      <td>-0.033634</td>\n",
              "      <td>-0.210305</td>\n",
              "      <td>0.038935</td>\n",
              "      <td>0.029853</td>\n",
              "      <td>0.004078</td>\n",
              "      <td>-0.209901</td>\n",
              "      <td>0.028864</td>\n",
              "      <td>-0.020685</td>\n",
              "      <td>-0.116714</td>\n",
              "      <td>-0.089824</td>\n",
              "      <td>-0.000962</td>\n",
              "      <td>-0.069954</td>\n",
              "      <td>-0.040098</td>\n",
              "      <td>0.052144</td>\n",
              "      <td>0.014680</td>\n",
              "      <td>0.069520</td>\n",
              "      <td>0.018773</td>\n",
              "      <td>-0.120430</td>\n",
              "      <td>-0.096468</td>\n",
              "      <td>0.036334</td>\n",
              "      <td>0.023615</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.018320</td>\n",
              "      <td>0.008598</td>\n",
              "      <td>-0.035451</td>\n",
              "      <td>0.050254</td>\n",
              "      <td>-0.067355</td>\n",
              "      <td>-0.035810</td>\n",
              "      <td>-0.083052</td>\n",
              "      <td>0.092934</td>\n",
              "      <td>-0.036804</td>\n",
              "      <td>0.049422</td>\n",
              "      <td>0.022433</td>\n",
              "      <td>0.036013</td>\n",
              "      <td>0.020027</td>\n",
              "      <td>0.106636</td>\n",
              "      <td>0.179138</td>\n",
              "      <td>-0.093071</td>\n",
              "      <td>0.029542</td>\n",
              "      <td>-0.083329</td>\n",
              "      <td>0.082547</td>\n",
              "      <td>-0.071713</td>\n",
              "      <td>0.012202</td>\n",
              "      <td>-0.013264</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>-0.291337</td>\n",
              "      <td>0.254224</td>\n",
              "      <td>-0.155738</td>\n",
              "      <td>-0.058512</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>-0.112091</td>\n",
              "      <td>0.014196</td>\n",
              "      <td>-0.116371</td>\n",
              "      <td>0.065155</td>\n",
              "      <td>0.173727</td>\n",
              "      <td>0.026280</td>\n",
              "      <td>-0.047481</td>\n",
              "      <td>-0.016186</td>\n",
              "      <td>0.131530</td>\n",
              "      <td>0.008204</td>\n",
              "      <td>-0.097958</td>\n",
              "      <td>...</td>\n",
              "      <td>0.096710</td>\n",
              "      <td>0.059853</td>\n",
              "      <td>0.171077</td>\n",
              "      <td>0.128387</td>\n",
              "      <td>-0.065607</td>\n",
              "      <td>-0.015473</td>\n",
              "      <td>0.089697</td>\n",
              "      <td>0.011542</td>\n",
              "      <td>-0.057754</td>\n",
              "      <td>0.037398</td>\n",
              "      <td>-0.001983</td>\n",
              "      <td>0.005053</td>\n",
              "      <td>0.060829</td>\n",
              "      <td>0.016896</td>\n",
              "      <td>0.042919</td>\n",
              "      <td>-0.144442</td>\n",
              "      <td>0.004372</td>\n",
              "      <td>0.075883</td>\n",
              "      <td>-0.037403</td>\n",
              "      <td>-0.142559</td>\n",
              "      <td>-0.002616</td>\n",
              "      <td>-0.017627</td>\n",
              "      <td>0.090729</td>\n",
              "      <td>-0.196299</td>\n",
              "      <td>0.055102</td>\n",
              "      <td>-0.073774</td>\n",
              "      <td>-0.033602</td>\n",
              "      <td>-0.132747</td>\n",
              "      <td>-0.059542</td>\n",
              "      <td>0.019749</td>\n",
              "      <td>-0.002796</td>\n",
              "      <td>-0.067205</td>\n",
              "      <td>0.100744</td>\n",
              "      <td>0.075183</td>\n",
              "      <td>0.039977</td>\n",
              "      <td>-0.048930</td>\n",
              "      <td>-0.125256</td>\n",
              "      <td>0.028781</td>\n",
              "      <td>-0.059022</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 769 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...       765       766       767  target\n",
              "0  0.004979  0.051084  0.011548  0.144239  ... -0.116885  0.005459  0.004883       3\n",
              "1 -0.008539  0.196301 -0.017056 -0.081347  ... -0.095894 -0.107140 -0.166406       2\n",
              "2  0.014586  0.010158  0.019481  0.139028  ... -0.133127  0.026859 -0.020756       2\n",
              "3  0.009449  0.057803 -0.032411  0.181052  ... -0.096468  0.036334  0.023615       1\n",
              "4 -0.018320  0.008598 -0.035451  0.050254  ... -0.125256  0.028781 -0.059022       7\n",
              "\n",
              "[5 rows x 769 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_5ts5zNKbAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_exceptions = np.where(train_camembert.mean(axis=1) == 0.)[0]\n",
        "\n",
        "for idx in idx_exceptions: \n",
        "    train_camembert.iloc[idx] = train_camembert.iloc[idx + 5] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4RFNeDRyzUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"pytorch_transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.metrics import make_scorer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEDCDuxay_OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loglikelihood_score(y_true, predictions, classes_order):\n",
        "    dic = {v:k for k, v in enumerate(classes_order)}\n",
        "    loss = 0\n",
        "    for i, cls in enumerate(y_true) :\n",
        "        loss -= np.log(predictions[i, dic[cls]])\n",
        "    loss = loss/len(y_true)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl6WPz0Qy4rC",
        "colab_type": "code",
        "outputId": "4b93e60c-3597-4991-ffa0-69391c83717b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train = train_camembert.iloc[:,:-1] \n",
        "y_train = train_camembert.target \n",
        "X_test = test_camembert.iloc[:, :]\n",
        "\n",
        "X_train = X_train.values \n",
        "y_train = y_train.values \n",
        "X_test = X_test.values\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1994, 768), (1994,), (560, 768))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7TgRgG3KnOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_1, X_2, Y_1, Y_2 = train_test_split(X_train, y_train, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHI9nj8OuVf9",
        "colab_type": "code",
        "outputId": "370d4539-83bd-4078-ce2b-0f3c6d0182d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "grid={\"C\":np.logspace(-1,3, num = 30)} \n",
        "\n",
        "logreg = LogisticRegression(solver='lbfgs',  multi_class='auto', max_iter=25000, n_jobs=-1) \n",
        "\n",
        "classes_order = LogisticRegression(solver='lbfgs',  multi_class='auto').fit(X_1[:, :2], Y_1).classes_ \n",
        "score_function = make_scorer(loglikelihood_score, greater_is_better=False, classes_order=classes_order, needs_proba=True) \n",
        "\n",
        "logreg_cv = GridSearchCV(logreg, grid, cv=3, verbose=3, n_jobs=-1, scoring=score_function) \n",
        "logreg_cv.fit(X_1, Y_1) \n",
        "\n",
        "print(logreg_cv.best_params_) \n",
        "print('Grid Search best score : ', logreg_cv.best_score_) \n",
        "print('Score on test', logreg_cv.score(X_2, Y_2) )"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   24.3s\n",
            "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  6.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 1.743328822199988}\n",
            "Grid Search best score :  -1.2459555009497325\n",
            "Score on test -1.3047290165347236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJbM_nxyFD1s",
        "colab_type": "code",
        "outputId": "32221cb2-19bc-4c17-8a43-a3c17d615f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'max_depth': [20, 40, 60],\n",
        "    'max_features': [2, 5, 10, 15, 20],\n",
        "    'n_estimators': [100, 200, 300, 1000]\n",
        "} \n",
        "\n",
        "# Create a based model\n",
        "rf = RandomForestClassifier() \n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, scoring=score_function,\n",
        "                          cv = 3, n_jobs = -1, verbose = 3) \n",
        "\n",
        "grid_search.fit(X_train, y_train) \n",
        "\n",
        "print(grid_search.best_params_) \n",
        "print('Score of Grid Search : ', grid_search.best_score_) \n",
        "print('Score on test', grid_search.score(X_2, Y_2) )"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   50.7s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  7.0min\n",
            "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 10.7min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:788: RuntimeWarning: invalid value encountered in subtract\n",
            "  array_means[:, np.newaxis]) ** 2,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'max_depth': 20, 'max_features': 2, 'n_estimators': 100}\n",
            "Score of Grid Search :  -inf\n",
            "Score on test -0.3673568805550964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdsZItdN1RT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62321a41-d8f9-4909-8ab5-9040436afd99"
      },
      "source": [
        "clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=0.7, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
        "             max_depth=6, min_child_weight=11, missing=-999, n_estimators=1000,\n",
        "             n_jobs=1, nthread=4, objective='multi:softprob', random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1337,\n",
        "             subsample=0.8, verbosity=1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print('Score on the Test set:',  clf.score(X_2, Y_2))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on the Test set: 0.9674185463659147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeDJEVunQjmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkd3fBwpQpue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6abdd3b1-6ae4-4adf-94e2-a76343a318f3"
      },
      "source": [
        "print('XGBoost score', accuracy_score(Y_2, clf.predict(X_2)))\n",
        "print('LR score', accuracy_score(Y_2, logreg_cv.best_estimator_.predict(X_2)))\n",
        "print('RF score', accuracy_score(Y_2, grid_search.best_estimator_.predict(X_2)))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBoost score 0.9674185463659147\n",
            "LR score 0.543859649122807\n",
            "RF score 0.9699248120300752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJF2B9cwVt3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('XGBoost score', log_loss(Y_2, clf.predict_proba(X_2)))\n",
        "print('Score of RF : ', log_loss(Y_2, logreg_cv.best_estimator_.predict_proba(X_2)))\n",
        "print('Score of LR :', log_loss(Y_2, grid_search.best_estimator_.predict_proba(X_2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNShJuyTFRqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write predictions to a file\n",
        "classes = ['business/finance','education/research','entertainment',\n",
        "  'health/medical','news/press','politics/government/law','sports','tech/science']\n",
        "\n",
        "write_submission(\"./embeds/test_submission_xgb_2.csv\", \n",
        "                 list(test_data[\"File\"]),\n",
        "                 model_classes_list=classes,\n",
        "                 predicted_probas= logreg_cv.best_estimator_.predict_proba(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87XT5AA1LeAe",
        "colab_type": "text"
      },
      "source": [
        "## Using Other features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFoQiFdRLvle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle \n",
        "from sklearn.preprocessing import OrdinalEncoder "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezkWf1c1L8Ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv('./embeds/' + 'train_noduplicates.csv', header = None) \n",
        "train_data.columns = ['File', 'Type'] \n",
        "\n",
        "test_data = pd.read_csv('./embeds/' + 'test.csv', header = None) \n",
        "test_data.columns = ['File'] \n",
        "\n",
        "enc = OrdinalEncoder() \n",
        "X = train_data['Type'] \n",
        "labels = enc.fit_transform(np.array(X).reshape(-1,1)) \n",
        "train_data['Labels'] = labels "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjOD_NEELd7T",
        "colab_type": "code",
        "outputId": "3e9ae93e-91a6-4055-9ae1-c0a94808177a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('./embeds/doc_vocab_embed.pickle', 'rb') as handle:\n",
        "  vocab_embedding_docs = pickle.load(handle)\n",
        "\n",
        "len(vocab_embedding_docs) "
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXKa9_d-LnZi",
        "colab_type": "code",
        "outputId": "602e3108-a431-4fd2-c0f4-bdf667b644a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "my_list = [] \n",
        "X = [] \n",
        "y = [] \n",
        "for element in vocab_embedding_docs.keys():\n",
        "  try:\n",
        "    if len(vocab_embedding_docs[element]) == 300:\n",
        "      y_t = train_data[train_data['File'] == int(element)]['Labels'].iloc[0]\n",
        "      y.append(y_t)\n",
        "      X.append(vocab_embedding_docs[element])\n",
        "  except:\n",
        "    my_list.append(element)\n",
        "\n",
        "X = np.vstack(X)\n",
        "y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "X_train = pd.DataFrame(X)\n",
        "Y_train = y.ravel()\n",
        "\n",
        "X_train.shape, Y_train.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1994, 300), (1994,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9YLNzAySHQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_1, X_2, Y_1, Y_2 = train_test_split(X_train, y_train, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBMrw8KEUnQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid={\"C\":np.logspace(-1,3, num = 30)}\n",
        "\n",
        "logreg = LogisticRegression(solver='lbfgs',  multi_class='auto', max_iter=25000, n_jobs=-1)\n",
        "\n",
        "classes_order = LogisticRegression(solver='lbfgs',  multi_class='auto').fit(x_train[:, :2], y_train).classes_\n",
        "score_function = make_scorer(loglikelihood_score, greater_is_better=False, classes_order=classes_order, needs_proba=True)\n",
        "\n",
        "logreg_cv = GridSearchCV(logreg,grid,cv=3, verbose=3, n_jobs=-1, scoring=score_function)\n",
        "\n",
        "logreg_cv.fit(X_1, Y_1)\n",
        "\n",
        "print(logreg_cv.best_params_)\n",
        "print('Score on the local test : ', logreg_cv.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glzr7WpXQS1K",
        "colab_type": "code",
        "outputId": "261fa054-4206-4ac5-d4bf-fe8b196601be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'max_depth': [20, 40, 60, 80, 100, 120]\n",
        "}\n",
        "\n",
        "# Create a based model\n",
        "rf = RandomForestClassifier()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, scoring=score_function,\n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "\n",
        "grid_search.fit(X_1, Y_1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   20.6s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:788: RuntimeWarning: invalid value encountered in subtract\n",
            "  array_means[:, np.newaxis]) ** 2,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators=100, n_jobs=None,\n",
              "                                              oob_score=False,\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'max_depth': [20, 40, 60, 80, 100, 120]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=make_scorer(loglikelihood_score, greater_is_better=False, needs_proba=True, classes_order=[0. 1. 2. 3. 4. 5. 6. 7.]),\n",
              "             verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVS67XVWWIRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Score of RF : ', accuracy_score(Y_2, grid_search.best_estimator_.predict_proba(X_2)))\n",
        "print('Score of LR :', accuracy_score(Y_2, logreg_cv.best_estimator_.predict_proba(X_2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIADQn1cUvlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2ed20f04-4f30-474b-e466-b6c21a5275d4"
      },
      "source": [
        "print('Score of RF : ', log_loss(Y_2, grid_search.best_estimator_.predict_proba(X_2)))\n",
        "print('Score of LR :', log_loss(Y_2, logreg_cv.best_estimator_.predict_proba(X_2)))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score of RF :  1.821928841396987\n",
            "Score of LR : 1.821928841396987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjtHdw-9UuTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write predictions to a file\n",
        "classes = ['business/finance','education/research','entertainment',\n",
        "  'health/medical','news/press','politics/government/law','sports','tech/science']\n",
        "\n",
        "write_submission(\"./Data/test_submission_rf_1.csv\", \n",
        "                 list(test_data[\"File\"]), \n",
        "                 model_classes_list=classes, \n",
        "                 predicted_probas=grid_search.best_estimator_.predict_proba(test_camembert))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}